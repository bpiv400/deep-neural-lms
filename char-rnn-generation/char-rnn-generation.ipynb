{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/eBRPvWB.png)\n",
    "\n",
    "# Practical PyTorch: Generating Shakespeare with a Character-Level RNN\n",
    "\n",
    "[In the RNN classification tutorial](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) we used a RNN to classify text one character at a time. This time we'll generate text one character at a time.\n",
    "\n",
    "```\n",
    "> python generate.py -n 500\n",
    "\n",
    "PAOLTREDN:\n",
    "Let, yil exter shis owrach we so sain, fleas,\n",
    "Be wast the shall deas, puty sonse my sheete.\n",
    "\n",
    "BAUFIO:\n",
    "Sirh carrow out with the knonuot my comest sifard queences\n",
    "O all a man unterd.\n",
    "\n",
    "PROMENSJO:\n",
    "Ay, I to Heron, I sack, againous; bepear, Butch,\n",
    "An as shalp will of that seal think.\n",
    "\n",
    "NUKINUS:\n",
    "And house it to thee word off hee:\n",
    "And thou charrota the son hange of that shall denthand\n",
    "For the say hor you are of I folles muth me?\n",
    "```\n",
    "\n",
    "This one might make you question the series title &mdash; \"is that really practical?\" However, these sorts of generative models form the basis of machine translation, image captioning, question answering and more. See the [Sequence to Sequence Translation tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) for more on that topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Reading\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and understand Tensors:\n",
    "\n",
    "* http://pytorch.org/ For installation instructions\n",
    "* [Deep Learning with PyTorch: A 60-minute Blitz](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb) to get started with PyTorch in general\n",
    "* [jcjohnson's PyTorch examples](https://github.com/jcjohnson/pytorch-examples) for an in depth overview\n",
    "* [Introduction to PyTorch for former Torchies](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb) if you are former Lua Torch user\n",
    "\n",
    "It would also be useful to know about RNNs and how they work:\n",
    "\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) shows a bunch of real life examples\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) is about LSTMs specifically but also informative about RNNs in general\n",
    "\n",
    "Also see these related tutorials from the series:\n",
    "\n",
    "* [Classifying Names with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) uses an RNN for classification\n",
    "* [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb) builds on this model to add a category as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1677881\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = open('../data/hilary_speeches.txt', encoding = 'UTF-8').read()\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make inputs out of this big string of data, we will be splitting it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r women’s work.\n",
      "I mean, think about it. It is 2016. This is not just a woman’s issue. If you’ve got a mother, a wife, a sister, a daughter, who is working, it’s your issue.\n",
      "Why should you be discrimina\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "This model will take as input the character for step $t_{-1}$ and is expected to output the next character $t$. There are three layers - one linear layer that encodes the input character into an internal state, one GRU layer (which may itself have multiple layers) that operates on that internal state and a hidden state, and a decoder layer that outputs the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 10\n",
      " 11\n",
      " 12\n",
      " 39\n",
      " 40\n",
      " 41\n",
      "[torch.LongTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        if string[c] in all_characters:\n",
    "            tensor[c] = all_characters.index(string[c])\n",
    "        else:\n",
    "            tensor[c] = 84\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can assemble a pair of input and target tensors for training, from a random chunk. The input will be all characters *up to the last*, and the target will be all characters *from the first*. So if our chunk is \"abc\" the input will correspond to \"ab\" while the target is \"bc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "To determine how our models evaluates the probability of certain in domain and out of domain texts, we calculate perplexity of some development document with the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(devfile_path):\n",
    "    full_input_text = open(devfile_path, encoding = 'UTF-8').read()\n",
    "    softmax = nn.Softmax()\n",
    "    full_input_tensor = char_tensor(full_input_text)\n",
    "    hidden = decoder.init_hidden()\n",
    "    prob = 0\n",
    "    for i in range(len(full_input_text)-1):\n",
    "        output, hidden = decoder(full_input_tensor[i], hidden)\n",
    "        output = softmax(output)\n",
    "        next_char = full_input_text[i+1]\n",
    "        if next_char in all_characters:\n",
    "            next_char = all_characters.index(next_char)\n",
    "        else:\n",
    "            next_char = 84\n",
    "        prob_next = math.log(output[0][next_char])\n",
    "        prob = prob + prob_next\n",
    "    prob = -1/(len(full_input_text)-1)*prob\n",
    "    prob = math.exp(prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper to print the amount of time passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the training parameters, instantiate the model, and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 21s (100 5%) 2.4250]\n",
      "Why ind cand afs lis[ guris jougo berat in thiey dous an dousin dume ancing hadry vame me gellis there \n",
      "\n",
      "[0m 40s (200 10%) 1.9901]\n",
      "Whtro, and thing thive whatige and verecternod gallen belle youids thonold the story helle all rimsers \n",
      "\n",
      "[0m 59s (300 15%) 2.1622]\n",
      "Wh maching I mow, bunth all wor Koono set[s and At and whearce molle gold ple to but leat Ucalion our  \n",
      "\n",
      "[1m 18s (400 20%) 1.9280]\n",
      "Whe and f desver apele and II thill lon wome and ef the Assally Fil of wealincally, re Ameye mages pea \n",
      "\n",
      "[1m 38s (500 25%) 1.6713]\n",
      "Whing are I whink under.\n",
      "Wat state a the beere nold it people of a whearday was we we nender stoowons  \n",
      "\n",
      "[1m 57s (600 30%) 1.6348]\n",
      "Who goes in and for. So a plalping that do to estatical do the to be on wito to peopines for ifer work \n",
      "\n",
      "[2m 16s (700 35%) 1.6873]\n",
      "Whill acal and unnesist a we sed turn inoman aply bave to the high [ to our us for becroding to he[s w \n",
      "\n",
      "[2m 35s (800 40%) 1.7049]\n",
      "Wh they stak f and to grate make the we[re can does.\n",
      "You keep us stire a swill refamed onorts we sold  \n",
      "\n",
      "[2m 55s (900 45%) 1.5607]\n",
      "When that the our lot for is on lointsing that this usniends. We dose thantran but the $00 sarybe the  \n",
      "\n",
      "[3m 14s (1000 50%) 1.7276]\n",
      "When Kamm what sees hears insues for have neesse unill make entorough mote where for prore was in I de \n",
      "\n",
      "[3m 34s (1100 55%) 1.6036]\n",
      "When many that we casing.\n",
      "Thered go soible, because were of mes Americake America to do want shoulds t \n",
      "\n",
      "[3m 53s (1200 60%) 1.8428]\n",
      "Whe will what back reaterity, the edicnted your childrent just of because frome peality the does in th \n",
      "\n",
      "[4m 12s (1300 65%) 2.0087]\n",
      "Wh Noctur Jonbides and the 8cting a fur-ause and the down in the want care the have that[s said, at th \n",
      "\n",
      "[4m 32s (1400 70%) 1.7064]\n",
      "When and that comess cheainning hem my got to give out ot thecess Bust commiting it combing thingee ou \n",
      "\n",
      "[4m 51s (1500 75%) 1.5180]\n",
      "When who that should right with our of failment the wath worken then works? Themele like more a for th \n",
      "\n",
      "[5m 10s (1600 80%) 1.7820]\n",
      "Whenged of commenity that itside to collarn Change Eusth all done on State, a ruppole new whive a coun \n",
      "\n",
      "[5m 30s (1700 85%) 1.7773]\n",
      "Wh there, nat[s, it our think out the the grate to rany destand, all of the come to press on young ase \n",
      "\n",
      "[5m 49s (1800 90%) 1.8414]\n",
      "When to morund to freen and doing to mere done to, hed is need to raise excrie.\n",
      "So I want orgat is it  \n",
      "\n",
      "[6m 8s (1900 95%) 1.9103]\n",
      "Which workers. We work and and or it[s are in exedry and the were that what of for no equaling contura \n",
      "\n",
      "[6m 28s (2000 100%) 1.5728]\n",
      "Whican ary then and Becurt on the bigy to boody and privers the crecing again the clually schools of c \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "lr = 0.005\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set())       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Training Losses\n",
    "\n",
    "Plotting the historical loss from all_losses shows the network learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11fae4e10>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ac0bc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating at different \"temperatures\"\n",
    "\n",
    "\n",
    "In the `evaluate` function above, every time a prediction is made the outputs are divided by the \"temperature\" argument. Using a higher number makes all actions more equally likely, and thus gives us \"more random\" outputs. Using a lower value (less than 1) makes high probabilities contribute more. As we turn the temperature towards zero we are choosing only the most likely outputs.\n",
    "\n",
    "We can see the effects of this by adjusting the `temperature` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thild. [Hight throse [ helping thank on the rassions a schools of country the creathers.\n",
      "And you his lifenaties look we still bett those be the economy reoplery of remofics preacther and those becoma or\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower temperatures are less varied, choosing only the more probable outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Than America the community the helpers and the country the country the country the community the country the security the have the community the community the helpers and here and the country the countr\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ther and the communities the helping the secuality the chance the country the because are so me and be should be and me communities the can the country the has and the security the country and he can an\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('th', 200, temperature = .3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanges the because with the commonical the people hand. You know what you can and debt because is and the contry and we care string or the heally so men secually all the the care have the clean be you \n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature = .5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cuntssurgn intove thinkemiis, Moballed deble which! Thenf much! Hey eswim [ fromuntecthing and it us thant valunatavifues together. He channon.\n",
      "Themberder theil gaise 5SS00% conncrione bougrach all \n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=1.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder Perplexity Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alicedeng/miniconda3/envs/deepenv/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernie Perp: 5.675883239939009\n",
      "Trump Perp: 8.081457773207417\n",
      "Warren Perp: 5.751003493524659\n",
      "Hilary Perp: 4.817371371037238\n",
      "Lincoln Perp: 8.846855707322247\n"
     ]
    }
   ],
   "source": [
    "lincoln_test_path = '../data/lincoln.txt'\n",
    "hilary_test_path = '../data/hilary.txt'\n",
    "warren_test_path = '../data/warren.txt'\n",
    "trump_test_path = '../data/trump.txt'\n",
    "bernie_test_path = '../data/bernie.txt'\n",
    "print('Bernie Perp: ' + str(perplexity(bernie_test_path)))\n",
    "print('Trump Perp: '+ str(perplexity(trump_test_path)))\n",
    "print('Warren Perp: '+ str(perplexity(warren_test_path)))\n",
    "print('Hilary Perp: '+ str(perplexity(hilary_test_path)))\n",
    "print('Lincoln Perp: '+ str(perplexity(lincoln_test_path)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises\n",
    "\n",
    "* Train with your own dataset, e.g.\n",
    "    * Text from another author\n",
    "    * Blog posts\n",
    "    * Code\n",
    "* Increase number of layers and network size to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next**: [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (HW6)",
   "language": "python",
   "name": "deepenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
